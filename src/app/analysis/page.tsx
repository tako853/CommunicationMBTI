'use client';

import { useState, useCallback, useRef } from 'react';
import { useRouter } from 'next/navigation';
import { WebcamCapture } from '@/components/WebcamCapture';
import { ScoreDisplay } from '@/components/ScoreDisplay';
import { ConversationLog } from '@/components/ConversationLog';
import { useFaceAnalysis } from '@/hooks/useFaceAnalysis';
import { useMediaPipe } from '@/hooks/useMediaPipe';
import { useAudioRecorder } from '@/hooks/useAudioRecorder';
import { useConversation } from '@/hooks/useConversation';
import { useTextToSpeech } from '@/hooks/useTextToSpeech';
import { calculateAllScores } from '@/services/scoreEngine';
import { analyzeSpeech } from '@/services/speechAnalysisService';
import { determineType } from '@/data/communicationTypes';
import type { TimelineEntry, CommunicationScores, CommunicationAxisScores, AnalysisResultData } from '@/types/analysis';

// ãƒ†ãƒ¼ãƒã‚«ãƒ©ãƒ¼
const theme = {
  primary: '#e24f29',      // ã‚ªãƒ¬ãƒ³ã‚¸ãƒ¬ãƒƒãƒ‰ï¼ˆã‚¢ã‚¯ã‚»ãƒ³ãƒˆï¼‰
  secondary: '#63a4a6',    // ãƒ†ã‚£ãƒ¼ãƒ«ã‚°ãƒªãƒ¼ãƒ³ï¼ˆã‚µãƒ–ã‚«ãƒ©ãƒ¼ï¼‰
  brown: '#7d6456',        // ãƒ–ãƒ©ã‚¦ãƒ³ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
  primaryLight: '#fef2ef', // ãƒ—ãƒ©ã‚¤ãƒãƒªã®è–„ã„èƒŒæ™¯
  secondaryLight: '#f0f7f7', // ã‚»ã‚«ãƒ³ãƒ€ãƒªã®è–„ã„èƒŒæ™¯
  brownLight: '#f7f5f4',   // ãƒ–ãƒ©ã‚¦ãƒ³ã®è–„ã„èƒŒæ™¯
};

type ConversationState = 'idle' | 'ai_speaking' | 'user_speaking' | 'processing';

export default function AnalysisPage() {
  const router = useRouter();
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [conversationState, setConversationState] = useState<ConversationState>('idle');
  const [scores, setScores] = useState<CommunicationScores>({
    expressiveness: 0,
    gestureActivity: 0,
    posturalOpenness: 0,
    eyeContact: 0,
    nodding: 0,
  });

  const timelineRef = useRef<TimelineEntry[]>([]);
  const userSpeechRef = useRef<string>('');
  const conversationStateRef = useRef<ConversationState>('idle');

  const faceAnalysis = useFaceAnalysis();
  const mediaPipe = useMediaPipe();
  const conversation = useConversation();
  const tts = useTextToSpeech();

  // æ²ˆé»™æ¤œå‡ºæ™‚ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’refã§ä¿æŒï¼ˆå¾ªç’°å‚ç…§å›é¿ï¼‰
  const handleSendMessageRef = useRef<() => void>(() => {});

  // æ²ˆé»™æ¤œå‡ºæ™‚ã«è‡ªå‹•é€ä¿¡
  const handleSilenceDetected = useCallback(() => {
    // user_speakingçŠ¶æ…‹ã®æ™‚ã®ã¿è‡ªå‹•é€ä¿¡
    if (conversationStateRef.current === 'user_speaking') {
      handleSendMessageRef.current();
    }
  }, []);

  const recorder = useAudioRecorder({
    silenceTimeout: 2000, // 2ç§’ã®æ²ˆé»™ã§é€ä¿¡
    onSilenceDetected: handleSilenceDetected,
  });

  const isLoading = faceAnalysis.isLoading || mediaPipe.isLoading;
  const isReady = faceAnalysis.isReady && mediaPipe.isReady;
  const error = faceAnalysis.error || mediaPipe.error || recorder.error || conversation.error || tts.error;

  const handleFrame = useCallback(
    async (video: HTMLVideoElement) => {
      await Promise.all([
        faceAnalysis.analyze(video),
        mediaPipe.analyze(video),
      ]);

      const entry: TimelineEntry = {
        timestamp: Date.now(),
        expressions: faceAnalysis.currentExpressions,
        pose: mediaPipe.currentPose,
        gesture: mediaPipe.currentGesture,
        headPose: mediaPipe.currentHeadPose,
        gaze: mediaPipe.currentGaze,
        handShape: mediaPipe.currentHandShape,
        bodyMovement: mediaPipe.currentBodyMovement,
      };

      timelineRef.current.push(entry);

      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ã‚³ã‚¢æ›´æ–°ï¼ˆæœ€æ–°30ã‚¨ãƒ³ãƒˆãƒªã§è¨ˆç®—ï¼‰
      const recentEntries = timelineRef.current.slice(-30);
      const newScores = calculateAllScores(recentEntries);
      setScores(newScores);
    },
    [faceAnalysis, mediaPipe]
  );

  // conversationStateã‚’æ›´æ–°ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼
  const updateConversationState = useCallback((state: ConversationState) => {
    setConversationState(state);
    conversationStateRef.current = state;
  }, []);

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—çµ‚ã‚ã£ãŸã‚‰é€ä¿¡
  const handleSendMessage = useCallback(async () => {
    updateConversationState('processing');

    try {
      // éŒ²éŸ³åœæ­¢â†’Whisperã§æ–‡å­—èµ·ã“ã—
      const userMessage = await recorder.stopAndTranscribe();

      if (!userMessage || userMessage.trim().length === 0) {
        // ç™ºè¨€ãŒãªã‘ã‚Œã°å†åº¦éŒ²éŸ³é–‹å§‹
        updateConversationState('user_speaking');
        await recorder.startRecording();
        return;
      }

      userSpeechRef.current += userMessage + '\n';

      // AIã®è¿”ç­”ã‚’å–å¾—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ã¾ã è¡¨ç¤ºã—ãªã„ï¼‰
      const aiText = await conversation.sendMessage(userMessage, true);
      updateConversationState('ai_speaking');

      // éŸ³å£°ã‚’å†ç”Ÿ
      await tts.speak(aiText);

      // éŸ³å£°å†ç”Ÿå¾Œã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
      conversation.addPendingMessage();

      // å†ã³ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ãƒ¼ãƒ³ï¼šéŒ²éŸ³é–‹å§‹
      updateConversationState('user_speaking');
      await recorder.startRecording();
    } catch (e) {
      console.error('Failed to send message:', e);
      updateConversationState('user_speaking');
      await recorder.startRecording();
    }
  }, [updateConversationState, recorder, conversation, tts]);

  // handleSendMessageã‚’refã«ç™»éŒ²
  handleSendMessageRef.current = handleSendMessage;

  // ä¼šè©±ã‚’é–‹å§‹
  const handleStartConversation = async () => {
    timelineRef.current = [];
    userSpeechRef.current = '';
    setIsAnalyzing(true);
    updateConversationState('ai_speaking');

    try {
      // AIãŒæœ€åˆã«è©±ã—ã‹ã‘ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ã¾ã è¡¨ç¤ºã—ãªã„ï¼‰
      const aiMessage = await conversation.startConversation(true);

      // éŸ³å£°ã‚’å†ç”Ÿ
      await tts.speak(aiMessage);

      // éŸ³å£°å†ç”Ÿå¾Œã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
      conversation.addPendingMessage();

      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ãƒ¼ãƒ³ï¼šéŒ²éŸ³é–‹å§‹
      updateConversationState('user_speaking');
      await recorder.startRecording();
    } catch (e) {
      console.error('Failed to start conversation:', e);
      updateConversationState('idle');
      setIsAnalyzing(false);
    }
  };

  // ä¼šè©±ã‚’çµ‚äº†
  const handleEndConversation = async () => {
    await recorder.stopRecording();
    tts.stop();
    setIsAnalyzing(false);
    updateConversationState('idle');
  };

  // çµæœã‚’åˆ†æ
  const handleAnalyze = async () => {
    setIsProcessing(true);

    try {
      // è»¸3ï¼ˆéè¨€èªã‚’ä¼ãˆã‚‹åŠ›ï¼‰ã¯æ˜ åƒã‹ã‚‰è¨ˆç®—
      // 5é …ç›®ã®åŠ é‡å¹³å‡: è¡¨æƒ…(25%) + ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼(25%) + å§¿å‹¢(20%) + ã‚¢ã‚¤ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ(20%) + é ·ã(10%)
      const finalScores = calculateAllScores(timelineRef.current);
      const nonverbalExpression = Math.round(
        finalScores.expressiveness * 0.25 +
        finalScores.gestureActivity * 0.25 +
        finalScores.posturalOpenness * 0.20 +
        finalScores.eyeContact * 0.20 +
        finalScores.nodding * 0.10
      );

      // è»¸1, 2, 4ã¯éŸ³å£°ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰GPTã§åˆ†æ
      let speechScores = {
        assertiveness: 50,
        listening: 50,
        nonverbalReading: 50,
        reasons: {
          assertiveness: '',
          listening: '',
          nonverbalReading: '',
        },
      };

      const userTranscript = conversation.getUserTranscript();
      const duration = conversation.getDuration();

      if (userTranscript.trim().length > 0) {
        try {
          speechScores = await analyzeSpeech(userTranscript, duration);
        } catch (e) {
          console.error('Speech analysis failed:', e);
        }
      }

      // å…¨è»¸ã®ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ
      const axisScores: CommunicationAxisScores = {
        assertiveness: speechScores.assertiveness,
        listening: speechScores.listening,
        nonverbalExpression,
        nonverbalReading: speechScores.nonverbalReading,
      };

      // ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
      const type = determineType(axisScores);

      // sessionStorageã«çµæœãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
      const resultData: AnalysisResultData = {
        type,
        axisScores,
        detailScores: finalScores,
        axisReasons: {
          assertiveness: speechScores.reasons.assertiveness,
          listening: speechScores.reasons.listening,
          nonverbalExpression: '', // æ˜ åƒåˆ†æã®ãŸã‚ç†ç”±ã¯è‡ªå‹•ç”Ÿæˆ
          nonverbalReading: speechScores.reasons.nonverbalReading,
        },
      };
      sessionStorage.setItem('analysisResult', JSON.stringify(resultData));

      // çµæœãƒšãƒ¼ã‚¸ã«é·ç§»
      router.push(`/result/${type}`);
    } finally {
      setIsProcessing(false);
    }
  };

  return (
    <div className="min-h-screen p-4">
      <div className="flex justify-center mb-4">
        <img
          src="/types/logo.jpg"
          alt="ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³MBTI"
          style={{ height: '60px', width: 'auto' }}
        />
      </div>

      {isLoading && (
        <div className="text-center p-8">
          <div className="text-lg" style={{ color: theme.brown }}>ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...</div>
        </div>
      )}

      {error && (
        <div className="text-center p-4 rounded mb-4" style={{ backgroundColor: theme.primaryLight, color: theme.primary }}>
          ã‚¨ãƒ©ãƒ¼: {error}
        </div>
      )}

      {isReady && (
        <div className="max-w-5xl mx-auto space-y-4">
          {/* ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: ä¼šè©±ãƒ­ã‚° + ã‚«ãƒ¡ãƒ©ï¼ˆå³æ¨ªï¼‰ */}
          <div className="flex gap-4 items-stretch">
            {/* ä¼šè©±ãƒ­ã‚°ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰ */}
            <div className="flex-1 rounded-lg shadow-sm p-4" style={{ backgroundColor: 'white', border: `1px solid ${theme.brown}20` }}>
              <div className="flex items-center justify-between mb-3">
                <h2 className="text-lg font-semibold" style={{ color: theme.brown }}>ä¼šè©±</h2>
                {/* çŠ¶æ…‹è¡¨ç¤º */}
                <div className="text-sm">
                  {(conversationState === 'ai_speaking' || tts.isSpeaking || conversation.isLoading) && (
                    <span className="font-medium" style={{ color: theme.primary }}>
                      {conversation.isLoading ? 'ğŸ¤” è€ƒãˆä¸­...' : 'ğŸ”Š AIãŒè©±ã—ã¦ã„ã¾ã™...'}
                    </span>
                  )}
                  {conversationState === 'user_speaking' && !tts.isSpeaking && !conversation.isLoading && (
                    <span className="font-medium" style={{ color: theme.secondary }}>
                      ğŸ¤ {recorder.hasSpeechStarted ? 'è‡ªå‹•é€ä¿¡å¾…ã¡...' : 'ãŠè©±ã—ãã ã•ã„'}
                    </span>
                  )}
                  {conversationState === 'processing' && !conversation.isLoading && (
                    <span className="font-medium" style={{ color: theme.brown }}>â³ æ–‡å­—èµ·ã“ã—ä¸­...</span>
                  )}
                </div>
              </div>
              <ConversationLog
                messages={conversation.messages}
                isLoading={conversation.isLoading}
                isSpeaking={tts.isSpeaking}
                isUserSpeaking={conversationState === 'user_speaking' && !tts.isSpeaking}
                isRecording={recorder.isRecording}
                hasSpeechStarted={recorder.hasSpeechStarted}
                isProcessing={conversationState === 'processing' && !conversation.isLoading}
              />
            </div>

            {/* ã‚«ãƒ¡ãƒ© + ãƒœã‚¿ãƒ³ï¼ˆå³æ¨ªï¼‰ */}
            <div className="flex-shrink-0 w-48 md:w-56 flex flex-col">
              {/* ã‚«ãƒ¡ãƒ© */}
              <div className="rounded-lg overflow-hidden shadow-lg relative" style={{ border: `2px solid ${theme.secondary}60` }}>
                <WebcamCapture
                  onFrame={handleFrame}
                  isAnalyzing={isAnalyzing}
                />
                {/* éŒ²éŸ³ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ */}
                {conversationState === 'user_speaking' && recorder.isRecording && (
                  <div className="absolute top-2 left-2 flex items-center gap-1 bg-black/50 px-2 py-1 rounded text-white text-xs">
                    <div className="w-2 h-2 rounded-full animate-pulse" style={{ backgroundColor: theme.primary }} />
                    REC
                  </div>
                )}
              </div>

              {/* æ“ä½œãƒœã‚¿ãƒ³ï¼ˆã‚«ãƒ¡ãƒ©ã®ä¸‹ï¼‰ */}
              <div className="flex-1 flex flex-col justify-end mt-3 gap-2">
                {!isAnalyzing ? (
                  <button
                    onClick={handleStartConversation}
                    disabled={isProcessing}
                    className="w-full text-white px-4 py-2 rounded-lg transition-opacity hover:opacity-90 disabled:opacity-50 text-sm"
                    style={{ backgroundColor: theme.secondary }}
                  >
                    ä¼šè©±ã‚’é–‹å§‹
                  </button>
                ) : (
                  <>
                    {conversationState === 'user_speaking' && (
                      <button
                        onClick={handleSendMessage}
                        className="w-full text-white px-4 py-2 rounded-lg transition-opacity hover:opacity-90 text-sm"
                        style={{ backgroundColor: theme.secondary }}
                      >
                        ç™ºè¨€ã‚’é€ä¿¡
                      </button>
                    )}
                    <button
                      onClick={handleEndConversation}
                      className="w-full text-white px-4 py-2 rounded-lg transition-opacity hover:opacity-90 text-sm"
                      style={{ backgroundColor: theme.brown }}
                    >
                      ä¼šè©±ã‚’çµ‚äº†
                    </button>
                  </>
                )}

                {conversation.messages.length > 0 && !isAnalyzing && (
                  <button
                    onClick={handleAnalyze}
                    disabled={isProcessing}
                    className="w-full text-white px-4 py-2 rounded-lg transition-opacity hover:opacity-90 disabled:opacity-50 text-sm"
                    style={{ backgroundColor: theme.primary }}
                  >
                    {isProcessing ? 'åˆ†æä¸­...' : 'çµæœã‚’è¦‹ã‚‹'}
                  </button>
                )}
              </div>
            </div>
          </div>

          {/* ã‚¹ã‚³ã‚¢è¡¨ç¤º */}
          <ScoreDisplay
            scores={scores}
            currentExpressions={faceAnalysis.currentExpressions}
            currentPose={mediaPipe.currentPose}
            currentGesture={mediaPipe.currentGesture}
            currentHeadPose={mediaPipe.currentHeadPose}
            currentGaze={mediaPipe.currentGaze}
            currentHandShape={mediaPipe.currentHandShape}
            currentBodyMovement={mediaPipe.currentBodyMovement}
          />
        </div>
      )}
    </div>
  );
}
